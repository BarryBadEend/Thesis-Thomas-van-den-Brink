{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b00f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = pd.read_csv(\"alternative_dataset_for_training.csv\")\n",
    "dataset = dataset.drop('Unnamed: 0', axis = 1)\n",
    "dataset = dataset.drop(['adviceUrgence.1','triage_urgentie_code.1'],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3462d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin edges for the 5-year intervals (e.g., 0-5, 5-10, ..., 70-75, etc.)\n",
    "bins = list(range(0, 105, 5))  # Change 80 to a higher value if needed\n",
    "\n",
    "# Create labels for each bin (e.g., '0-5', '5-10', etc.)\n",
    "bin_labels = [f'{i}-{i+5}' for i in range(0, 100, 5)]\n",
    "\n",
    "# Use pd.cut to create a new column 'age_category' based on the bins\n",
    "dataset['age_category'] = pd.cut(dataset['age'], bins=bins, labels=bin_labels, right=False)\n",
    "\n",
    "# One-hot encode the 'age_category' column\n",
    "df_one_hot = pd.get_dummies(dataset['age_category'], prefix='age')\n",
    "\n",
    "# Join the one-hot encoded columns back to the original dataframe\n",
    "dataset = pd.concat([dataset, df_one_hot], axis=1)\n",
    "\n",
    "# Drop the original 'age_category' column if not needed\n",
    "dataset.drop('age_category', axis=1, inplace=True)\n",
    "dataset = dataset.drop('age', axis =1)\n",
    "#dataset = dataset.loc[:,~dataset.columns.str.endswith('.1')].columns\n",
    "dataset = dataset[dataset.loc[:,~dataset.columns.str.endswith('.1')].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33321c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertaal_df = pd.read_csv(\"UTF-8Vertaling_Ids_Vragen\").drop(\"Unnamed: 0\", axis = 1)\n",
    "vertaal_df['Id'] = \"Q_\"+vertaal_df['Id'].astype(str)\n",
    "vertaal_df\n",
    "mapping = {row[\"Id\"]: f'{row[\"Title\"]} ({row[\"Id\"]})' for _, row in vertaal_df.iterrows()}\n",
    "\n",
    "# Rename the columns in dataset\n",
    "dataset = dataset.rename(columns=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3abbf011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Maak_Lokale_Dataset(Triage_MINDD, Escalation, Dataset, Lichaamsdeel= None):\n",
    "    Local_subset = Dataset.copy()\n",
    "    Local_subset['adviceUrgence'] = Local_subset['adviceUrgence'].str.replace('U', '').astype(int)\n",
    "    Local_subset['triage_urgentie_code'] = Local_subset['triage_urgentie_code'].str.replace('U', '').astype(int)\n",
    "    if Escalation == \"DeEscalation\":\n",
    "        Local_subset =  Local_subset[((Local_subset['adviceUrgence'].astype(int) == Triage_MINDD) & (Local_subset['triage_urgentie_code'].astype(int) >= Triage_MINDD))]\n",
    "        Local_subset['Escalatie'] = Local_subset['triage_urgentie_code'].apply(lambda x:-1 if x<Triage_MINDD else (0 if x==Triage_MINDD else 1))\n",
    "\n",
    "    if Escalation == \"Escalation\":\n",
    "        Local_subset =  Local_subset[((Local_subset['adviceUrgence'].astype(int) == Triage_MINDD) & (Local_subset['triage_urgentie_code'].astype(int) <= Triage_MINDD))]\n",
    "        Local_subset['Escalatie'] = Local_subset['triage_urgentie_code'].apply(lambda x:1 if x<Triage_MINDD else (0 if x==Triage_MINDD else -1))\n",
    "\n",
    "    Local_subset = Local_subset.loc[:, (Local_subset != 0).any(axis=0)]\n",
    "    if Lichaamsdeel != None:\n",
    "        lichaamsdeelnaam = \"bodyAreaTitle_\"+Lichaamsdeel\n",
    "        Local_subset = Local_subset[Local_subset[lichaamsdeelnaam]==1]\n",
    "    Local_subset = Local_subset.replace({\"no\":-1,\"not asked\":0,\"yes\":1})\n",
    "    Local_subset = Local_subset.loc[:,(Local_subset != 0).any(axis =0)]\n",
    "    Local_subset = Local_subset.replace({False:0,True:1})\n",
    "    # Substring to search for in column names\n",
    "    substring = 'Q_'  # Change this to your desired substring\n",
    "    \n",
    "    # Filter columns whose names contain the substring\n",
    "    columns_to_filter = [col for col in Local_subset.columns if substring in col]\n",
    "\n",
    "    # Count occurrences of 1 and -1 in the selected columns\n",
    "    count_ones_and_negatives = Local_subset[columns_to_filter].apply(lambda col: (col == 1).sum() + (col == -1).sum())\n",
    "\n",
    "    # Keep only columns where the count of 1s and -1s is at least 2\n",
    "    columns_to_drop = count_ones_and_negatives[count_ones_and_negatives < 5].index\n",
    "\n",
    "    # Drop the filtered columns\n",
    "    df_filtered = Local_subset.drop(columns=columns_to_drop)\n",
    "\n",
    "    return df_filtered\n",
    "    return Local_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520decc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model(Triage_MINDD, Escalation, X_train, y_train):\n",
    "    from sklearn.linear_model import LogisticRegression    \n",
    "    logreg = LogisticRegression(max_iter = 100000, class_weight = \"balanced\", solver = \"liblinear\")\n",
    "    logreg.fit(X_train, y_train)\n",
    "    return logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16dd780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model_SVM(Triage_MINDD, Escalation, X_train, y_train):\n",
    "    from sklearn import svm\n",
    "    svm = svm.SVC(kernel = 'linear', C=0.1, max_iter = -1, gamma = 10, class_weight = \"balanced\")\n",
    "    svm.fit(X_train, y_train)\n",
    "    return svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d417713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model_EBM(Triage_MINDD, Escalation, X_train, y_train):\n",
    "    from interpret.glassbox import ExplainableBoostingClassifier\n",
    "    ebm = ExplainableBoostingClassifier(feature_types = ['nominal' for number in range(len(X_train.columns))],smoothing_rounds = 100,max_rounds = 1000000000, learning_rate = 0.02, interactions = 0.99, interaction_smoothing_rounds = 5, inner_bags = 20)\n",
    "    from sklearn.utils.class_weight import compute_sample_weight\n",
    "    weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "    ebm.fit(X_train, y_train, sample_weight = weights)\n",
    "    return ebm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa8fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model_DT(Triage_MINDD, Escalation, X_train, y_train):\n",
    "    from sklearn import tree\n",
    "    dt = tree.DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 5, class_weight = \"balanced\")\n",
    "    dt.fit(X_train, y_train)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso, lasso_path\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assume X_train, X_test, X_validation, y_train, y_test, y_validation are defined and share the same features\n",
    "def logistic_regression_simplification(model, X_train, X_test, y_train, y_test): \n",
    "# Train an SVM with a linear kernel\n",
    "    logreg = model\n",
    "    #\n",
    "    # Get feature weights from the trained linear SVM\n",
    "    coef = logreg.coef_.copy()\n",
    "\n",
    "    # Get raw term contributions\n",
    "    X_trn_tc = X_train * coef  # Element-wise multiplication\n",
    "    X_tst_tc = X_test * coef\n",
    "\n",
    "    # LASSO path over logreg term contributions\n",
    "    alphas, coefs, _ = lasso_path(X_trn_tc, y=y_train, positive=True)\n",
    "\n",
    "    # Evaluate each alpha using AUC-ROC\n",
    "    aucs = []\n",
    "    for coef_lasso in coefs.T:\n",
    "        lasso_pred = X_tst_tc @ coef_lasso\n",
    "        auc = roc_auc_score(y_test, lasso_pred)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    # Select best alpha based on max AUC\n",
    "    best_idx = np.argmax(aucs)\n",
    "    best_coef = coefs[:, best_idx]\n",
    "\n",
    "    # Identify selected features (non-zero LASSO coefficients)\n",
    "    non_zero_mask = best_coef != 0\n",
    "    selected_features = X_train.columns[non_zero_mask]\n",
    "\n",
    "    # Filter datasets to keep only selected features\n",
    "    X_train_filtered = X_train[selected_features]\n",
    "    X_test_filtered = X_test[selected_features]\n",
    "\n",
    "    # Retrain SVM on filtered dataset\n",
    "    logreg_filtered = LogisticRegression(max_iter = 100000, solver = \"liblinear\",class_weight = \"balanced\")\n",
    "    logreg_filtered.fit(X_train_filtered, y_train)\n",
    "    \n",
    "    return logreg_filtered, X_test_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5806813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Lasso, lasso_path\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assume X_train, X_test, X_validation, y_train, y_test, y_validation are defined and share the same features\n",
    "def SVM_simplification(model, X_train, X_test, y_train, y_test): \n",
    "\n",
    "    svm = model\n",
    "    # Get feature weights from the trained linear SVM\n",
    "    coef = svm.coef_.copy()\n",
    "\n",
    "    # Get raw term contributions\n",
    "    X_trn_tc = X_train * coef  # Element-wise multiplication\n",
    "    X_tst_tc = X_test * coef\n",
    "\n",
    "    # LASSO path over SVM term contributions\n",
    "    alphas, coefs, _ = lasso_path(X_trn_tc, y=y_train, positive=True)\n",
    "\n",
    "    # Evaluate each alpha using AUC-ROC\n",
    "    aucs = []\n",
    "    for coef_lasso in coefs.T:\n",
    "        lasso_pred = X_tst_tc @ coef_lasso\n",
    "        auc = roc_auc_score(y_test, lasso_pred)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    # Select best alpha based on max AUC\n",
    "    best_idx = np.argmax(aucs)\n",
    "    best_coef = coefs[:, best_idx]\n",
    "\n",
    "    # Identify selected features (non-zero LASSO coefficients)\n",
    "    non_zero_mask = best_coef != 0\n",
    "    selected_features = X_train.columns[non_zero_mask]\n",
    "\n",
    "    # Filter datasets to keep only selected features\n",
    "    X_train_filtered = X_train[selected_features]\n",
    "    X_test_filtered = X_test[selected_features]\n",
    "\n",
    "    # Retrain SVM on filtered dataset\n",
    "    svm_filtered = SVC(kernel = 'linear', C=0.1, max_iter = -1, gamma = 10, class_weight = \"balanced\")\n",
    "    svm_filtered.fit(X_train_filtered, y_train)\n",
    "\n",
    "    return svm_filtered, X_test_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "472ba567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def simplify_ebm_with_logistic_reweighting(ebm_model,X_train,X_test,y_train,y_test,C_grid=None,keep_intercept=True,verbose=True,cv_folds=5):\n",
    "    C_grid = np.logspace(-3, 2, 10)  # From 0.001 to 100\n",
    "\n",
    "    # Get EBM term contributions\n",
    "    X_train_terms = ebm_model.eval_terms(X_train)\n",
    "    X_test_terms = ebm_model.eval_terms(X_test)\n",
    "\n",
    "    # === Cross-validation to find best C ===\n",
    "    best_C = None\n",
    "    best_score = -np.inf\n",
    "    auc_scores = []\n",
    "\n",
    "    for C in C_grid:\n",
    "        clf = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        C=C,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    clf.fit(X_fold_train, y_fold_train)\n",
    "    y_pred = clf.predict_proba(X_fold_val)[:, 1]\n",
    "\n",
    "    if len(np.unique(y_fold_val)) < 2:\n",
    "        continue  # Skip invalid fold\n",
    "\n",
    "    auc = roc_auc_score(y_fold_val, y_pred)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "    # === Refit with best C on full training set ===\n",
    "    clf = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        C=best_C,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    clf.fit(X_train_terms, y_train)\n",
    "    coef = clf.coef_[0]\n",
    "    # Clone the model\n",
    "    simplified_ebm = copy.deepcopy(ebm_model)\n",
    "\n",
    "    # Apply weights to EBM term functions\n",
    "    for idx, weight in enumerate(coef):\n",
    "        simplified_ebm.scale(idx, factor=weight)\n",
    "\n",
    "    simplified_ebm.intercept_ = float(clf.intercept_)\n",
    "\n",
    "    simplified_ebm.sweep()\n",
    "    return simplified_ebm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9503def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "def simplifiy_decision_trees(model, X_train, X_test, y_train, y_test):\n",
    "    DT = model\n",
    "    models = []\n",
    "    path = DT.cost_complexity_pruning_path(X_train,y_train)\n",
    "    alphas, impurities = abs(path.ccp_alphas), path.impurities\n",
    "    for alpha in alphas:\n",
    "        clf = tree.DecisionTreeClassifier(ccp_alpha=alpha, max_depth = 5,min_samples_leaf = 5, class_weight = \"balanced\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        models.append(clf)\n",
    "    test_scores = [roc_auc_score(y_test,clf.predict(X_test)) for clf in models]\n",
    "    max_score = np.max(test_scores)\n",
    "    candidate_alphas = alphas[test_scores == max_score]\n",
    "    best_alpha = np.max(candidate_alphas)\n",
    "    simplified_DT = tree.DecisionTreeClassifier(ccp_alpha = best_alpha, max_depth = 5, min_samples_leaf = 5, class_weight = \"balanced\")\n",
    "    simplified_DT.fit(X_train, y_train)\n",
    "    return simplified_DT\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f3eaac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "def custom_10_fold(dataset, model_type, Triage_MINDD, Escalation):\n",
    "    local_dataset = Maak_Lokale_Dataset(Triage_MINDD, Escalation, dataset)\n",
    "    kf = KFold(n_splits = 10)\n",
    "    X_data = local_dataset.drop(['triage_urgentie_code','adviceUrgence','Escalatie'],axis=1)\n",
    "    y_data = local_dataset['Escalatie']\n",
    "    roc_scores = []\n",
    "    f1_scores = []\n",
    "    recall_scores = []\n",
    "    roc_simplified_scores = []\n",
    "    f1_simplified_scores = []\n",
    "    recall_simplified_scores = []\n",
    "    n_params = []\n",
    "    n_params_simplified = []\n",
    "    for train_index, test_index in kf.split(local_dataset):\n",
    "        X_train, X_test = X_data.iloc[train_index], X_data.iloc[test_index]\n",
    "        y_train, y_test = y_data.iloc[train_index], y_data.iloc[test_index]\n",
    "        if model_type == \"logreg\":\n",
    "            model = Create_model(Triage_MINDD, Escalation, X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "            f1 = f1_score(y_test, predictions)\n",
    "            recall = recall_score(y_test, predictions)\n",
    "            roc_auc = roc_auc_score(y_test, predictions)\n",
    "            f1_scores.append(f1)\n",
    "            recall_scores.append(recall )\n",
    "            roc_scores.append(roc_auc)\n",
    "            n_param = len(model.coef_[0])\n",
    "            n_params.append(n_param) \n",
    "            simplified_model, X_test_filtered = logistic_regression_simplification(model,X_train, X_test, y_train, y_test)\n",
    "            simplified_predictions = simplified_model.predict(X_test_filtered)\n",
    "            f1_simplified = f1_score(y_test, simplified_predictions)\n",
    "            recall_simplified = recall_score(y_test, simplified_predictions)\n",
    "            roc_auc_simplified = roc_auc_score(y_test, simplified_predictions)\n",
    "            f1_simplified_scores.append(f1_simplified)\n",
    "            recall_simplified_scores.append(recall_simplified)\n",
    "            roc_simplified_scores.append(roc_auc_simplified)\n",
    "            n_param_simplified = len(simplified_model.coef_[0])\n",
    "            n_params_simplified.append(n_param_simplified)\n",
    "        if model_type == \"SVM\":\n",
    "            model = Create_model_SVM(Triage_MINDD, Escalation, X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "            f1 = f1_score(y_test, predictions)\n",
    "            recall = recall_score(y_test, predictions)\n",
    "            roc_auc = roc_auc_score(y_test, predictions)\n",
    "            f1_scores.append(f1)\n",
    "            recall_scores.append(recall)\n",
    "            roc_scores.append(roc_auc)\n",
    "            n_param = len(model.coef_[0])\n",
    "            n_params.append(n_param) \n",
    "            simplified_model, X_test_filtered = SVM_simplification(model, X_train,X_test, y_train, y_test)\n",
    "            simplified_predictions = simplified_model.predict(X_test_filtered)\n",
    "            f1_simplified = f1_score(y_test, simplified_predictions)\n",
    "            recall_simplified = recall_score(y_test, simplified_predictions)\n",
    "            roc_auc_simplified = roc_auc_score(y_test, simplified_predictions)\n",
    "            f1_simplified_scores.append(f1_simplified)\n",
    "            recall_simplified_scores.append(recall_simplified)\n",
    "            roc_simplified_scores.append(roc_auc_simplified)\n",
    "            n_param_simplified = len(simplified_model.coef_[0])\n",
    "            n_params_simplified.append(n_param_simplified)\n",
    "        if model_type == \"DT\":\n",
    "            model = Create_model_DT(Triage_MINDD, Escalation, X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "            f1 = f1_score(y_test, predictions)\n",
    "            recall = recall_score(y_test, predictions)\n",
    "            roc_auc = roc_auc_score(y_test, predictions)\n",
    "            f1_scores.append(f1)\n",
    "            recall_scores.append(recall)\n",
    "            roc_scores.append(roc_auc)\n",
    "            n_param = sum(model.tree_.children_left != -1)\n",
    "            n_params.append(n_param)\n",
    "            simplified_model = simplifiy_decision_trees(model, X_train,X_test, y_train, y_test)\n",
    "            simplified_predictions = simplified_model.predict(X_test)\n",
    "            f1_simplified = f1_score(y_test, simplified_predictions)\n",
    "            recall_simplified = recall_score(y_test, simplified_predictions)\n",
    "            roc_auc_simplified = roc_auc_score(y_test, simplified_predictions)\n",
    "            f1_simplified_scores.append(f1_simplified)\n",
    "            recall_simplified_scores.append(recall_simplified)\n",
    "            roc_simplified_scores.append(roc_auc_simplified)\n",
    "            n_param_simplified = sum(simplified_model.tree_.children_left != -1)\n",
    "            n_params_simplified.append(n_param_simplified)\n",
    "        if model_type == \"EBM\":\n",
    "            model = Create_model_EBM(Triage_MINDD, Escalation, X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "            f1 = f1_score(y_test, predictions)\n",
    "            recall = recall_score(y_test, predictions)\n",
    "            roc_auc = roc_auc_score(y_test, predictions)\n",
    "            f1_scores.append(f1)\n",
    "            recall_scores.append(recall)\n",
    "            roc_scores.append(roc_auc)\n",
    "            n_param = len(model.term_names_)\n",
    "            n_params.append(n_param)\n",
    "            simplified_model = simplify_ebm_with_logistic_reweighting(model, X_train,X_test, y_train, y_test)\n",
    "            simplified_predictions = simplified_model.predict(X_test)\n",
    "            f1_simplified = f1_score(y_test, simplified_predictions)\n",
    "            recall_simplified = recall_score(y_test, simplified_predictions)\n",
    "            roc_auc_simplified = roc_auc_score(y_test, simplified_predictions)\n",
    "            f1_simplified_scores.append(f1_simplified)\n",
    "            recall_simplified_scores.append(recall_simplified)\n",
    "            roc_simplified_scores.append(roc_auc_simplified)\n",
    "            n_param_simplified = len(simplified_model.term_names_)\n",
    "            n_params_simplified.append(n_param_simplified)\n",
    "            \n",
    "    return roc_scores, f1_scores, recall_scores, n_params, roc_simplified_scores, f1_simplified_scores, recall_simplified_scores, n_params_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4cb74-28f2-4ba7-9795-9060bd55b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import csv\n",
    "all_possible_combinations = [(1, \"DeEscalation\"),(2,\"DeEscalation\"),(2,\"Escalation\"),(3,\"DeEscalation\"),(3,\"Escalation\"),(4, \"DeEscalation\"),(4, \"Escalation\"),(5,\"Escalation\")]\n",
    "all_model_types = [\"logreg\"]\n",
    "for type in all_model_types:\n",
    "    all_roc_scores = []\n",
    "    all_f1_scores = []\n",
    "    all_recall_scores = []\n",
    "    all_n_params = []\n",
    "    all_roc_simplified_scores = []\n",
    "    all_f1_simplified_scores = []\n",
    "    all_recall_simplified_scores = []\n",
    "    all_n_params_simplified = []\n",
    "    \n",
    "    for combo in all_possible_combinations:\n",
    "        print(combo)\n",
    "        roc_scores, f1_scores, recall_scores, n_params, roc_simplified_scores, f1_simplified_scores, recall_simplified_scores, n_params_simplified = custom_10_fold(dataset, type, combo[0], combo[1])\n",
    "        all_roc_scores.append(roc_scores)\n",
    "        all_f1_scores.append(f1_scores)\n",
    "        all_recall_scores.append(recall_scores)\n",
    "        all_n_params.append(n_params)\n",
    "        all_roc_simplified_scores.append(roc_simplified_scores)\n",
    "        all_f1_simplified_scores.append(f1_simplified_scores)\n",
    "        all_recall_simplified_scores.append(recall_simplified_scores)\n",
    "        all_n_params_simplified.append(n_params_simplified)\n",
    "    file_name = str(type) +\"_fold_results.csv\"\n",
    "    rows = zip(all_possible_combinations,all_roc_scores,all_f1_scores,all_recall_scores,all_n_params, all_roc_simplified_scores, all_f1_simplified_scores, all_recall_simplified_scores,all_n_params_simplified)\n",
    "    with open(file_name,'w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(['model','all_roc_scores','all_f1_scores','all_recall_scores','all_n_params','all_roc_simplified_scores', 'all_f1_simplified_scores','all_recall_simplified_scores','all_n_params_simplified'])\n",
    "        csv_out.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
